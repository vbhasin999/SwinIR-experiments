{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23066,
     "status": "ok",
     "timestamp": 1730476059078,
     "user": {
      "displayName": "Vedant Bhasin",
      "userId": "16901580205563811674"
     },
     "user_tz": 240
    },
    "id": "ERyI6BHTZa5a",
    "outputId": "111fc811-546a-4a57-d4a6-dcca84079a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: coremltools in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (8.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from coremltools) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from coremltools) (5.28.2)\n",
      "Requirement already satisfied: sympy in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from coremltools) (1.13.2)\n",
      "Requirement already satisfied: tqdm in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from coremltools) (4.66.5)\n",
      "Requirement already satisfied: packaging in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from coremltools) (24.1)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from coremltools) (24.2.0)\n",
      "Requirement already satisfied: cattrs in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from coremltools) (24.1.2)\n",
      "Requirement already satisfied: pyaml in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from coremltools) (24.9.0)\n",
      "Requirement already satisfied: PyYAML in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from pyaml->coremltools) (6.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from sympy->coremltools) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: timm in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (1.0.9)\n",
      "Requirement already satisfied: torch in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from timm) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from timm) (0.17.2)\n",
      "Requirement already satisfied: pyyaml in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from timm) (0.25.2)\n",
      "Requirement already satisfied: safetensors in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: requests in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from huggingface_hub->timm) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torch->timm) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: numpy in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: thop in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from thop) (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torch->thop) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torch->thop) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torch->thop) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torch->thop) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torch->thop) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from torch->thop) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from jinja2->torch->thop) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from sympy->torch->thop) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vedantbhasin/opt/anaconda3/envs/odml_env/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install coremltools\n",
    "%pip install timm\n",
    "%pip install opencv-python\n",
    "%pip install thop\n",
    "%pip install pandas\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4022,
     "status": "ok",
     "timestamp": 1730476109719,
     "user": {
      "displayName": "Vedant Bhasin",
      "userId": "16901580205563811674"
     },
     "user_tz": 240
    },
    "id": "iXRjp0Z5igor"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "config = {\n",
    "    'classical_sr' : {\n",
    "        'args': {\n",
    "            'upscale' : 2, # flexible\n",
    "            'in_chans' : 3,\n",
    "            'img_size' : 48,\n",
    "            'window_size' : 8,\n",
    "            'img_range' : 1.,\n",
    "            'depths' :[6, 6, 6, 6, 6, 6],\n",
    "            'embed_dim' : 180,\n",
    "            'num_heads' : [6, 6, 6, 6, 6, 6],\n",
    "            'mlp_ratio' : 2,\n",
    "            'upsampler' : 'pixelshuffle',\n",
    "            'resi_connection' : '1conv',\n",
    "        },\n",
    "        'path' : 'model_zoo/swinir/001_classicalSR_DIV2K_s48w8_SwinIR-M_x2.pth', #match with upscale factor\n",
    "    },\n",
    "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "\n",
    "    'lightweight_sr' : {\n",
    "        'args': {\n",
    "            'upscale' : 2, # flexible\n",
    "            'in_chans' : 3,\n",
    "            'img_size' : 64,\n",
    "            'window_size' : 8,\n",
    "            'img_range' : 1.,\n",
    "            'depths' :[6, 6, 6, 6],\n",
    "            'embed_dim' : 60,\n",
    "            'num_heads' : [6, 6, 6, 6],\n",
    "            'mlp_ratio' : 2,\n",
    "            'upsampler' : 'pixelshuffledirect',\n",
    "            'resi_connection' : '1conv',\n",
    "        },\n",
    "        'path' : 'model_zoo/swinir/002_lightweightSR_DIV2K_s64w8_SwinIR-S_x2.pth', #match with upscale factor\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12773,
     "status": "ok",
     "timestamp": 1730476126386,
     "user": {
      "displayName": "Vedant Bhasin",
      "userId": "16901580205563811674"
     },
     "user_tz": 240
    },
    "id": "wD3uV5i0h2Ca",
    "outputId": "661a480f-4c3f-4337-bf66-bb9e1c63bd81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinIR(\n",
       "  (conv_first): Conv2d(3, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (patch_unembed): PatchUnEmbed()\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=60, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.004)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.009)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.013)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.017)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.022)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "    (1): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=60, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.026)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.030)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.035)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.039)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.043)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.048)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "    (2): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=60, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.052)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.061)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.065)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.070)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.074)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "    (3): RSTB(\n",
       "      (residual_group): BasicLayer(\n",
       "        dim=60, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.078)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.083)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.087)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.096)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
       "            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=60, window_size=(8, 8), num_heads=6\n",
       "              (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (patch_embed): PatchEmbed()\n",
       "      (patch_unembed): PatchUnEmbed()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "  (conv_after_body): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upsample): UpsampleOneStep(\n",
       "    (0): Conv2d(60, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): PixelShuffle(upscale_factor=2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.network_swinir import SwinIR as net\n",
    "\n",
    "device = config['device']\n",
    "TASK_NAME = 'lightweight_sr'\n",
    "\n",
    "\n",
    "model_info = config[TASK_NAME]\n",
    "model = net(**model_info['args'])\n",
    "param_key_g = 'params'\n",
    "\n",
    "pretrained_model = torch.load(model_info['path'])\n",
    "model.load_state_dict(pretrained_model[param_key_g] if param_key_g in pretrained_model.keys() else pretrained_model, strict=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pruning_utils' from '/Users/vedantbhasin/Library/CloudStorage/GoogleDrive-vbhasin999@gmail.com/My Drive/SwinIR-experiments/pruning_utils.py'>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import pruning_utils as pu\n",
    "importlib.reload(pu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "overall sparsity: 0.0000\n",
      "model sparsity: 0.0000\n",
      "model size: 34.07 MB\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.prune import global_unstructured, L1Unstructured\n",
    "import copy\n",
    "mod2prune = copy.deepcopy(model)\n",
    "\n",
    "parameters_to_prune = [\n",
    "    (module, name) for module in mod2prune.modules()\n",
    "    for name, param in module.named_parameters(recurse=False)\n",
    "]\n",
    "\n",
    "orig_params = copy.deepcopy(parameters_to_prune)\n",
    "n_iter = 0\n",
    "for itr in range(n_iter):\n",
    "    global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=L1Unstructured,\n",
    "        amount=0.33\n",
    "    )\n",
    "\n",
    "print(f'\\noverall sparsity: {pu.calculate_sparsity_overall(mod2prune, parameters_to_prune):.4f}')\n",
    "print(f'model size: {pu.calculate_sparse_model_size(mod2prune, temp_file=f\"pruned_models/unstructured_itr_{n_iter}\"):.2f} MB')\n",
    "\n",
    "# pruning_data_unstructured['Iteration'].append(itr + 1)\n",
    "# pruning_data_unstructured['Sparsity'].append(pu.calculate_sparsity_overall(mod2prune, parameters_to_prune))\n",
    "# pruning_data_unstructured['Disk Size (MB)'].append(pu.calculate_sparse_model_size(mod2prune))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model sparsity: 0.0000\n",
      "model size: 34.07 MB\n"
     ]
    }
   ],
   "source": [
    "print(f'model size: {pu.calculate_sparse_model_size(model):.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_data_unstructured = {\n",
    "    \"Iteration\" : [0, 1, 2, 3, 4, 5],\n",
    "    \"Sparsity\" : [0.0, 0.33, 0.5511, 0.6992, 0.7985, 0.8650],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Latency (s)\" : [],\n",
    "    \"Disk Size (MB)\" : [34.07, 27.23, 22.52, 19.56, 17.65, 16.40],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "odml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
